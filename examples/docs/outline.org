,

* Raft Enablement Process

The two examples directories share a common ancester and a lot of the code is 
identical or near to it. Here are the differences:

** Two examples
*** The "counters" example

This example is structured to serve as a living guide for how to add
raft consensus to an application. It is also intended to have a decent chance
of serving well as a skeleton on which to build your own application, especially
the early stages of trying things out. In particular,there are tools to help
create and manage clusters, especially test clusters running on a single machine
as would typically be desireable while doing development tasks.

*** The "options" example
This example is pretty much the same application as the "counters" example,
but it is organized to use different RPC mechanisms and Raft log implementations. This
might be helpful if you want to develop your own log or rpc mechanism, you should be
able to follow the pattern for choosing among the existing "options" and add your own.
You could do this separately from developing your actual application "state machine"
(as the raft documents call it).

This example is more cumbersome to operate than the "counters" example, as it branched
off from an earlier common ancestor and fails to benefit from later refinements.
However, in addition to offering the log and rpc
choices, it also has support for performance testing including tools to compare the
effect of the available choices. That could be especially useful for measuring your
own log or rpc components.


*** Structure of examples

If you imagine that you are starting out with some code that already works in a single process,
where you have some core functionality that is available through an internal API, maybe the
methods on a class, and you want to turn this into a replicated cluster with Raft guarantees,
this is a process that you could use and which is exemplifed by the code here. 

1. Write some kind of validator code to run against the "target" API. Maybe a demo too
   if you like that sort of thing (I do).  
2. Write a mechanism that duplicates the "target" API as a "proxy" API and places
   serialize/deserialize tools between Them. Why this is necessary is described below.
3. Build and RPC or message passing mechanism that wraps the serialize and deserialize tools
   and makes it possible for the "proxy" API and the "target" API live in different processes
   or even on different machines.
4. Build a Raft Server and Raft Client layer that is based on the Raftengine library
   that implements the required APIs and uses the library's components to add
   raft consensus and replication between the "proxy" and "target" APIs.

These examples are built to retain all these stages of development so that they can serve as
examples of how this stepwise layering of functionality could work. It turns out that
it can be usefull to retain the structure of the earlier stages for debugging and
performance analysis, so you might want to do the same when building your raft system.


** Example "Base" Code

In each example the src/base directory contains this code, broken up into
these components

*** Application Components:

The main application component is *"counters.py"*. This is the home of
the target API for these applications. This is a trivial mechanism for
maintaining a dictionary of counters, where each key in the dictionary
is restricted to a single characther from the python
string.ascii_letters set and each value in the dictionary is an
integer, initialized to 0. The entire API implementation is as
follows:
 
#+BEGIN_SRC python
    async def counter_add(self, name, value):
        if len(name) != 1 or name[0] not in string.ascii_letters:
            raise Exception('Counter names must be a single ascii letter')
        self.counts[name] += value
        self.save()
        return self.counts[name]

    async def get_counters(self):
        return dict(self.counts)
#+END_SRC

The "self.save()" call saves a pickled copy of the "counts" dictionary to disk,
thus making this a persistent state machine.

*** Test and Demo Components

1. demo.py -  This does a simple demo of the counters functions. Since it is coded to accept
   an object that implements the API without regard to the details of that object,
   it should continue to work as a demo as additional layers of functionality
   are added in later steps.

2. validator.py - Whereas the demo code does very little checking of the provided
   functions, and produces output that is not always desireable, this component
   provides some basic functional validation of the API implementaion and avoids output.
   Just like the demo code, is coded to accept an object that implements the API
   without regard to the details of that object, so it should continue to work as
   a demo as additional layers of functionality are added in later steps.

*** Executable components

These executable tools run the code from the demo and validator components,
doing the housekeeping needed to support them. This is pretty thin on functionality,
but the analogs in future steps will have more to do.

1. demo_counters.py - runs the demo code
2. test_counters.py - runs the validator code

You can run either of these with no command line options. They require no libraries not provided by python.
   
** Example "Split Base" Code

This code, in src/split_base, adds the serializer/deserializer functions that are
needed to enable your application API to participate in Raft consensus and replication.
You application operations that need to be Rafted must be accessible via a serialized
"state machine command" as explained in the Raft Protocol definition. There a probably
a very large number of ways you can do this. These examples us a very simple JSON
based scheme with simple string "command" names. Whatever method you want to use, this
is the place to build it, so you can build fetures iteratively if you need to, easily
debug the code, etc.

These examples use this terminolgy:

1. The "Collector" implements that "target" API as a "proxy" API. In these examples
   the two are identical, which helps enable clean progression from one development
   stage to the next. You don't have to do it this way, if you need additional
   functions at the "proxy" level, but such functions probably belong in a layer
   added later.
2. The "Dispatcher" accepts an implementation of the actual "target" API and
   translates the serialized commands into calls to the "state machine" portion
   of the app.

*** Layer Main Components

1. collector.py contains the Collector for these examples
2. dispatcher.py contains the Dispatcher for these examples

*** Support Components

1. "pipe.py" The connection between the Collector and Dispatcher in this layer is called
   a "pipe", and it serves as a scaffold for connecting the two prior to adding RPC functionality.

*** Executable components

These executable tools run the code from the base demo and validator components but with
the Collector and Dispatcher (and pipe) components inserted between the demo and test components
and the target API implemention. The construct all the application components, the layer components
and the support components and wire them up. The demo and test components should run without change.

1. demo_split.py - runs the demo code
2. test_split.py - runs the validator code
   
You can run either of these with no command line options. They require no libraries not provided by python.
   
** Example "RPC" Code

Both the "counters" and "options" examples have a src/rpc directory, and they serve
the same purpose and have some overlapping features. However, the "counters" example
uses only one RPC mechanism, and the "options" example supports three different ones.
This section will describe the general structure and features that are common.
Additional detail about the "options" is described in a different section.

The "counter" RPC mechanism is one built for these examples. It uses python async
socket operations directly rather than using an external library. It is also one
of the RPC options available in the "options" example, where it is labeled "astream"
for async streams. The other RPC components available the "options" example are
aiozrpc and gRPC, more about them in their specific section.

All the RPC components share the same basic API, with some additions in some cases.

*** Layer Main Components

1. RPCCLient in rpc_client.py - supports the basic RPC API:
   1. Initialized with a host and port, will connect to a server when the first async call is made
   2. async closer() method that closes any open socket and cleans up any resources
   3. issue_command - This is the transport for an application state machine command. This mirrors
      the method used in the "FakeRPCPipe" in the src/split_base code. So this will become a
      Raft mediated request in the final stage.
   4. raft_message - This is the RPC method that the Raft servers will use to send each other messages
      for the Raft protocol. 
   5. direct_server_command - This is a helper RPC that lets admin clients request that the server do things
      such as reporting status, shut down, etc. You don't have to have this RPC in your app, but you
      will probably need something like this.
2. RPCServer in rpc_server.py - Implements the server side of the issue_command, raft_message and direct_server_command RPC,
   along with doing all the needed housekeeping to support clients. Passes issue_command, raft_message
   and direct_server_command messages on to the "raft_server" instance that is supplied to its init function.
      
*** Support Components

The RaftServerStub component in "raft_stub.py" is the target of the RPCServer class issue_command,
raft_message and direct_server_command messages, in lieu of the RaftServer class that will be used in the
next stage. It has an instance of a Dispatcher class, which in turn has a Counters instance, so
it can complete the serialized operations passed in issue_command. It stubs out the raft_message
calls with an echo of the incomming message. It implements the actual direct commands that the
direct_server_command RPC targets.

The "RunTools" class in "run_tools" provides helper functions that create client and server instances
for testing in the test and demo executables. They are not used outside this purpose.


*** Executable components

These executable tools use the RunTools class from "run_tools.py" to setup a server and a client
and wire everything together so that the base Demo and Validator classes can run over the RPC
connection between the two.

1. demo_rpc.py - Creates an RPCServer instance, and an RPCClient instance and runs the base Demo code
2. test_rpc.py - Creates an RPCServer instance, and an RPCClient instance and runs the base Validator code

** Example "Raft" Code

The "src/raft" directory contains the components needed to add Raft support to the RPC level of code.
This includes an implementation of the Raftengine PilotAPI and a layer of client and server. 

*** Layer Main Components

1. RaftClient in "raft_client.py", provides a thin layer on top of RPCClient. The main purpose
   of this layer is to automatically deal with the "redirct" and "retry" responses andthat can occur when
   making "issue_command" calls. This "redirect" response happens when the server to which the RPC
   client is connected is not the cluster Leader, in which case the RaftClient will use the redirect
   information to connect to the leader and resend the issue_command request. The "retry" response
   happens when the cluster is currently leaderless, which almost always means that an election is in
   progress. Upon "retry" the RaftClient will wait a bit, then retry the issue_command call and repeat
   until there is a leader or the retry count reaches max_retries.
2. RaftServer in "raft_server.py", connects Raftengine components together with a Pilot instance (below)
   to construct the Raft enablement for state machine commands accepted from issue_command RPCs, and
   Raft support by connecting message received via the raft_message RPCs to the Raftengine. It includes
   initialization of:
   1. The RPCserver
   2. The SqliteLog, a Raftengine LogAPI implementation found in the raftengine_logs repo
   3. The Dispatcher, same dance as in the src/rpc and src/split_base layers.
   4. A Counters implementation, though in this case it relies on external code to provide a reference
      to the class instead of importing it, which will make more sense with details the "ops" layer.
   5. The Pilot class (see below), which requires a reference to the SqliteLog and Dispatcher instances
   6. The Raftengine Deck class, which requires a refeverence to the Pilot class as it makes calls to
      the PilotAPI
3. Pilot in "pilot.py", implements that PilotAPI with operations which include:
   1. Routing commands to the Dispatcher when called by the Raftengine upon commit of command containg
      log records.
   2. Accepting raft messages and messages replies and routing them to other raft servers via RPCClient
      (note, not RaftClient) instance that it manages automatically.
   3. SnapShot operations by calls to the Counters class (see executables below)


*** Support Components

The DirectCommander class in "direct.py" provides the lookup and execution of
direct_server_command operations. The DirectCommander it provided to the RPCServer
on startup. The available commands include "status", "getpid", "stop", "get_leader"
and "take_power". The last command is only needed when the user decides to start
the cluster with slow timeouts to aid in development tasks, an will then need
to trigger an election by sending this command to a server.


*** Executable Components

Servers at this layer require a cluster definition to start up so that they know how many
servers are in the cluster, as needed for Raft consensus. The executable components in
this directory assume that you will only configure three servers for the cluster, and that
all of them will run on the local machine. Additional choices are available in the next
layer.

Note that none of the code in this layer attempts to do snapshot operations. The executables
wire the src/base/counters.py Counters class into the RaftServer, and this Counter
class does not implement snapshot API elements. The next layer adds this feature.

**** Cluster Configuration Options

There are two methods of configuring a cluster available. One method is used when you want
to run servers directly in a terminal, and the other is when you want to run the servers
in the background. In either event, a working directory is created in /tmp for each
server and the cluster configuration is written to a file in that directory. When the server
starts it will create (or reload) its SqliteLog file in that directory. 

1. Direct server run, using "run_server.py". When you start a server this way, it creates a
   server configuration based on the command line options "--base_port" and "--index". The
   base_port option determines the port used for the uri of the first server in the cluster.
   A cluster config is created starting with that port and then two more servers are configured
   with the next two port numbers. So "--base_port 40000" would yield a cluster with uris:
   sum://127.0.0.1:40000, sum://127.0.0.1:40001 and sum://127.0.0.1:40002.
   The --index value tell the server which of those uris it should use.
2. Backround run of a cluster, using the Cluster class in "cluster.py", a cluster can be configured,
   the working directory set up, and the run_server.py executable started for each configured
   server. There is no command line tool to do this. The demo_raft.py and test_raft.py tools
   use these features. Should you want to do this directly, you could build a command line
   tool that uses the Cluster class, but there is extensive support for cluster management
   via single command line operations and an interactive command loop available at the next
   layer, which also includes the full range of cluster operations such as membership changes
   and snapshots.

**** Test/Demo components

Each of these create and start a three server cluster, run their ops, then stop the cluster.
   
1. demo_raft.py - Runs the existing Demo code against a Collector that uses the RaftClient class.
2. test_ratt.py - Runs the existing Validator code against a Collector that uses the RaftClient class.

   


** Example "Ops" Code

This layer adds the support you would need in the "operations" role, managing a cluster of
Raft enabled servers. The core Raft related features it adds are cluster membership change
operations and snapshot operations.

Actual runtime management of a cluster of servers requires at least a few administrative
tools, and this layer adds the minimum required set of those. This includes tools
to query the status of a cluster, find, use and modify clusters configurations on a machine,
find clusters on other machines, start and stop servers in a cluster, add and remove
servers, manage snapshots.

The tools provided by this layer are by no means a complete and finished set, but they
are adequate for most development and testing scenarios. A true production level toolset
would need to do more, but could be based on what is provided here.


*** Layer Main Components

The "raft_counters.py" file contains an extension of the base Counters class. This extension
adds support for creating and loading snapshots and sharing them between servers. This
example application is so simple that the snapshot process is dead simple and does not
provide much of a guide to more complicated snapshot scenarios, but then that is probably
not possible to usefully discusss without reference to the application design. The basic
idea is, save whatever is needed in order to restore the current state of the application.

The details of the snapshot process supported by the Raftengine snapshot propogation
logic is documented in the library's docs.

*** Support Components

1. DirectCommander in "direct.py" provides the actions needed to manage a server in the
   cluster, including the commands from the previous level, plus commands to collect more
   information such as the LogStats value accessible via the LogAPI. It also adds
   access to features in the Raftengine Deck class such as join and exiting a cluster.
   There is also a DirectComamndClient class in this file that provides an API to
   send the direct command requests and handle the results, in some cases converting
   dictionaries to dataclass instances.
2. A variety of cluster and server administration utilities are found in "admin_common.py"
3. The setup and run portions of a server exectuable are found in "server_procs.py".
   


*** App/Test Executable Components

The usual demo and test operations are here as from other layers, and the both use
the "test_common.py" contents to perform some of their functions, since cluster creation
and control operations are complex and common needs of each.

1. In "demo_ops.hy" the usual "Demo" class operations are run against a Collector that
   works via a RaftClient, but the main function (literally "main") is in "test_common.py"
2. In "test_ops.py" the Validator operations run just like Demo above, but after the normal
   test run a snapshot test is also run.
3. Should you wish to run a server directly from the command line, the "run_server.py" tool
   does that in a way that is compatible with the configuration methods in this directory.
  
   

*** Cluster Management Executable Components

A set of operations that conduct cluster configuration and operations tasks are
encapsulated in the ClusterMgr class in "cluster_mgr.py". There is a command line
interface in "cluster_cmd.py" that allows execution of these operations by single command
line invocation. Some of the options to this tool specify how to find and select a
cluster configuration before performing the requested operation(s). This tool can also
be used to launch and interactive command lopp, found in "command_loop.py".

Although the above tool can create cluster configurations, it is sometimes desirable
to create them directly, and "create_cluster.py" fills that requirement.


As such it has
some abstractions and complexity that you might not want to retain in your production
code base. There may be some value in keeping some of it, since having a single process
implementation that connects your client and server code can be useful for debugging
when the problem is not related to the functions of the remoting method (RPCs or whatnot),
or indeed to find out if it is related.

Anywho, the structure is important to preserving clarity in this demo where there
are multiple transport options and multiple versions of the app with varying degrees
of completeness of the stepwise process.

If you are starting from scratch in building your client server app that will use the
Raftengine library to add Raft consensus support, this directory lays out a process
for building it by layering functionality build sequentially. If you already have
a client server application that you are going to enhance, most of this process
is still relevant, you would just start in the middle somewhere based on the
degree of completeness, probably at major step 3.

The demo application here is a simple banking simulation that operates on customer
and account records that are stored in a Sqlite database.

* Development Major Steps

** Step 1. Build your application functionality in with three components.

These components are precursors of functions that later steps will perform
to deliver RPC access to the Operations. Separating them this way greatly
aids the later stages of development


1. Operations - methods that implement your applications operations. In Raft
   terminology, this will be the "state machine". 
2. Proxy - a facade or wrapper that matches the Operations method signatures
   that will serve as a layer of separation between the Operations and the Client
3. Client - The class that application user code will interact with to get
   access to the Operation functions.
4. Build a single process step tool that connects these components together for use,
   providing a function or method that returns a Client instance that is prepared
   for use.
5. Build a validator that uses the setup tool to get a Client, and then ensures
   that calls to the Client reach the Operations and return expected results. 


** Step 2. Add RPC functionality to your components


1. Add a RPC server, a wrapper that provides RPC interface access to all the Operations
   methods.
2. Write a new Proxy implementation that uses RPC client calls to talk to the server.
3. Build a setup tool that provides a server instance for running as a process, and a client
   instance for connecting to it.
4. Construct a new version of the validator that uses the RPC enabled client and server to
   validate that all operations work.

   
** Step 3. Add a set of components to make Operations access indirect.

Starting with the Step 1 code base, add these components:

1. A Collector component that implements the Proxy interface but takes
   each method call and converts it and its arguments to a serialized
   form as a string.
2. A Dispatcher component that takes a serialized method call and
   converts it to a call to the Operations component.
3. A temporary connector component that moves Collector packets to
   the Dispatcher and returns results.

Then:

1. Build a setup tool that connects Client to Collector (serving as Proxy) which
   is connected to connector which is connected to Dispatcher which is connected
   to Operations.
2. Construct a validator that checks the operations of the composed components.

** Step 4. Combine the results of step2 and step3 and prepare the Raft message RPC

1. Edit the components as needed to move the Collector to the RPC server and
   wire the Collector and Dispatcher as in step 3.
2. Update the RPC interface to add a "raft_message" RPC that takes and returns
   a string.
3. Configure client and server setup as in step 2.
4. Build a validator that is like the one in step2.
   
** Step 5. Integrate Raftengine library.
