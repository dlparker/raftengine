

* Raft Enablement Process

This demo is built to retain all the stages of development so that they can serve as
examples of how this stepwise layering of functionality could work. As such it has
some abstractions and complexity that you might not want to retain in your production
code base. There may be some value in keeping some of it, since having a single process
implementation that connects your client and server code can be useful for debugging
when the problem is not related to the functions of the remoting method (RPCs or whatnot),
or indeed to find out if it is related.

Anywho, the structure is important to preserving clarity in this demo where there
are multiple transport options and multiple versions of the app with varying degrees
of completeness of the stepwise process.

If you are starting from scratch in building your client server app that will use the
Raftengine library to add Raft consensus support, this directory lays out a process
for building it by layering functionality build sequentially. If you already have
a client server application that you are going to enhance, most of this process
is still relevant, you would just start in the middle somewhere based on the
degree of completeness, probably at major step 3.

The demo application here is a simple banking simulation that operates on customer
and account records that are stored in a Sqlite database.

The code here preserves the state of the code at the end of each of the major
steps detailed below, so that you can review how the steps may be applied to
your project.

This part of the document summarizes the major steps of the recommended
process. More detailed descriptions may be found in the next section.

* Major steps

** Major step one

1. Build your server operations functionality and define the API the client will use.
2. Build your client functionality for using the server operations API.
3. Build an in-process version of your app that allows the client to call
   the operations with direct access, though through a proxy.
4. Build a validator that ensures that the client gets the expected results
   from the operations.

** Major step two

1. Build a transport and a proxy to manage it for the client.
2. Run the validator to ensure that the transport provides full access to the server, operations.


** Major step three

Raft enabled operations require that your server operations be accessible via log replication. This
means that your request to the server needs to be stored in a Raft Log record, then replicated
across the cluster, and only after a majority of servers have saved the record, then applied to
underlying operational logic.

Your operations methods therefore need to be accessible by converting a serialized repsentation of the
method identity and the method arguments that can be deserialized later and use to call the operations logic.

A peculiarity of this design is that the results of the serialized command do not need to be
serialized. The client will only ever receive a result from the cluster leader, and the cluster
leader server is the same one that receives the full RPC from the client, so it can return the
result in original python object form. The non-leader servers will apply the serialized command
and simply discard the results. 

This might make the process a little hard to understand, so it is a good idea to build the
infrastructure for performing this conversion and testing it directly before integrating it with
the RPC mechanisms, much less the Raftengine library.

The final target form of this process will have the client calling the server operatiosn API methods via
the RPCClient to send RPCs to the Raft Leader server which will then serialize the RPC request and deliver it
to the Raftengine library. The Raftengine library will save the request in the Raft log and then
replicate the log record to the other servers. Once enough servers have received the log record the
Leader will mark the record committed, then it will apply the command to the local Operations logic.
The follower servers will soon learn that the record is committed and will retrieve the command
from the log record an apply it to the local Operations logic.

So in this architecture the RPCs are never directly connect to the Operations logic in the server.
Even at the Leader server the Raft library logic applies the command to the Operations logic just
as it does in the folowers.

For the purposes of this discussion the server component that receives the RPC is call a Collector,
as it collects all the RPC signatures into a single serialize command format and passes it on to
the Raft Libary. The component that applies that serialized command to the Operations is called
the Dispatcher.

Major step three is the development of the Collector and Dispatcher.

1. Build a Collector class that implements the OpsProxyAPI and that converts the method
   calls and arguments of that API to JSON compatible serialized command string.
2. Build a Dispatcher that unpacks the command and calls the actual Operations methods
   with the unpacked arguments, and directly returns the results.
3. To verify before moving on the the next step, and to easy verification and debugging
   of future changes in API signatures, build a single process test of the Client to
   Collector to Dispatcher to Operations flow and test all the operations.

The direct in-process version of the demo of this step is in step3/direct.

For assurance that the code can work properly across processes, this step in the demo
also includes a simple TCP socket RPC mechanism to support the single RPC in step3/astream.

In this exercise the Collector is in the client whereas it would normally be in the server. It
uses the TCP RPC client to send the condensed command to the Dispatcher in the server, via
the TCP RPC server. In the final version the Collector would be in one server and the Dispatcher
will be accessed via Raftengine library operations either in other servers or in the Leader
server by retrieving the command from the log.


** Major step four

Major step four is to build RPC support for the Raft libary via a new RPC that passes
a single string argument, which will be a JSON encodeed Raft message, possibly including
your command, but Raft has other uses for these messages as well.

There is a choice to be made about how this RPC fits into the overall architecture. The two
possibilities are to encorporate the RPC into your existing set, the OperationsAPI, or to
make it a separate RPC interface. There are arguments on both sides.

1. Separate RPC interfaces:
   1. Pros
      1. Network admins can use separate networks for server to server versus client to server
	 which may benefit security, performance an reliability engineering concerns.
      2. Should it be desirable to add a different RPC mechanism for client to server operations,
	 it would not be necessary to replace the server to server mechanism. This would greatly
	 ease a phased rollout of the change since the cluster would keep operating normally
	 as you switched the server population over to the new client RPC mechanism
      3. It is slightly less confusing to developers because there is no overlap of usage. Clients
	 call servers with the first RPC interface but never the second, and servers call each other
	 with the second but never the first.
   2. Cons
      1. More complex since you need to build and manage two RPC servers, or at least one server with two
	 interfaces.
      2. Each server needs two network endpoints instead of one.
      3. Could be more confusing to developers depending on how much code structure is needed to organize
	 and support the two interfaces.
2. Singe RPC interface
   1. Pros
      1. Implementation is simpler, just add an RPC to existing interface.
      2. Slightly reduces the likelihood that you'll have maintenance headaches with changes
	 to the serialized datatypes.
      3. Code structure is much simpler
   2. Cons
      1. Possibly a bit confusing to developers because clients processes are clients of all the RPCs
	 except the Raft message RPC, and servers are clients of the Raft message RPCs but not
	 of all the other RPCs in the same interface.
	 



   
3. Update the client, proxy, server and transport to support the Raft Message operations
   which require a single additional RPC that transports an encoded raft message and
   receives an encoded raft reply. You can call it what you like,
   a natural name is "raft_message". This will be used by the servers for the message
   traffic that is needed to manage the cluster, hold elections, replicate log records,
   everything that the Raft protocol does that requires interaction between the nodes. 
4. Build a set of tools that convert your API calls and their arguments into byte or string encoded
   data.
   1. For each API function, define a method call data structure that contains identifies the API function and
      its arguments.
   2. Build an encoder to convert the method call data structure to string or bytes
   3. Build a decoders for the method call data structures
5. Build a ProxyWrapper class that implements your application's OpsProxyAPI that converts
      the methods to encode using your new encoders.
6. Build a Dispatcher class that decodes your messages into specific server calls and returns the result to
      the caller.
7. Build a special TestFacade that implements OpsProxyAPI that will only be used for
   this stage of development and will  be discarded when the Raftengine library is
   integrated in a later step. This will be used in the server, not the client.
   1. Provide it with the server instance and an instance of the ProxyWrapper class and an instance
      of the Dispatcher class.
   2. Write the methods so that they:
      1. Use the ProxyWrapper to convert the method call to a serialized request
      2. Pass the serialized request to the Dispatcher 
      3. Collect the result of the Dispatcher call and return it.
   3. Build a server that uses this TestFacade in place of the actual server
8. Run the client validator against this TestFacade server


* Major step one details

** Build the Server and the Proxy API

In the demo in src/base/server.py and src/base/proxy_api.py

This server has no functions except those that the client will
trigger with RPCs. Your server may differ, but this
discussion only considers RPC support and Raft support.

The OpsProxyAPI is just a convenience to ensure you
provide all the methods in your RPCClient class. This
can be helpful when moving from one step in this process
to the next. You can discard the OpsProxyAPI class
when development is done and replace it with your
concreate RPCClient class.

** Build your client.

In the demo in src/base/client.py

Our client does nothing except call the Server methods via the OpsProxyAPI,
but it is common for real world clients to do other things such as caching
handles, emitting events, etc.

   
** Build a validator client function or class

In demo in src/base/test_banking.py

This serves as a basic sanity check for ensuring that the
client can access the server functionaly. This should
be built to accept a client instance and not contain any
variant specific behavior so that it can be used by all
variants.

** Build a direct in-process version of the app

In demo in src/direct/proxy.py.

Build a concrete implementation of the proxy api that has direct access to a server object.
The OpsProxyAPI is intended for use in the client side, so normally it would
not be in the same process as the server, but here that is the goal, to skip using
any actual RPCs and use the proxy instance as a substitute. 


** Configure a test the direct version

*** Build a SetupHelperAPI implementation.

In demo in src/direct/setup_helper.py

You need a SetupToolAPI implementation to inform the command line tools
how to setup the application. See [Architecture of multi-stage development support tools]

In this case only one method will be used and only by the client
command line tools, and it will configure the server and the proxy.

** Configure a test of the direct version

In the demo we have support for multiple transports and multiple
completion stages, so we add some configuration to
src/cli/transports.py to identify the SetupHelper for the direct
case.

The tools in src/cli/run_server.py and src/cli/run_client.py offer
the choice of variant via command line arguments. For the direct
variant run_server.py -t direct will tell you to just run the client.
run_client -t direct will create a server, a proxy and a client
and wires them together, then runs the validator tool against the client.

You can build something simpler than this, but keep in mind that you
might want to retain the ability to run the direct mode in order to
simplify error analysis and debugging even after your raft enabled
servers are in production. I can be challenging to debug servers that
can change cluster leadership based on timeouts.


*** Architecture of multi-stage development support tools

This demo is built to retain all the stages of development so that they can serve as
examples of how this stepwise layering of functionality could work. As such it has
some abstractions and complexity that you might not want to retain in your production
code base. There may be some value in keeping some of it, since having a single process
implementation that connects your client and server code can be useful for debugging
when the problem is not related to the functions of the remoting method (RPCs or whatnot),
or indeed to find out if it is related.

Anywho, the structure is important to preserving clarity in this demo where there
are multiple transport options and multiple versions of the app with varying degrees
of completeness of the stepwise process.

To ensure the each stage does not need duplicate implementations of the various
command line tools needed for testing and development, we use a layer of abstraction
to connect the command line tools with the particular you wan to use. The
key element if this is the src/base/setup_helper.py SetupHelperAPI class. It
defines tools for creating servers, proxies, clients, etc. via method
calls.

Each variant builds an implementation of the SetupHelper that
addresses the specific needs of that variant. The command line tools
have a hardcoded registry of the available variants and the
needed setup tool and chooses the right one based on user input.


* Major step two details

The details of this step are dependent upon what RPC mechanism you decide to use. The
demo contains three one of which will hopefully be close enough to your own choice
to see how the same process can apply to your choice. If you are using something not
demonstrated here, you might want to add another demo variant just to make sure your
choice is compatible with the process. In partcular, you can get into issues if your
server code is async and your RPC mechanism is not. 

** Step Two Implementation Examples

The demo includes three complete RPC transport implementations in =src/step2/= to demonstrate
different approaches to making the bank demo a distributed app. Each implementation follows
the same architectural pattern but uses different RPC mechanisms.

*** Available RPC Transports

**** aiozmq (Zero Message Queue)
- *Location*: =src/step2/aiozmq/=
- *Protocol*: TCP with MessagePack serialization
- *Use Case*: High-performance, low-latency messaging
- *Benefits*: Excellent for server-to-server Raft communication, minimal overhead
- *Command*: =python src/cli/run_server.py -t step2_aiozmq -p 55555=

**** gRPC (Google Remote Procedure Call)
- *Location*: =src/step2/grpc/=
- *Protocol*: HTTP/2 with Protocol Buffers + MessagePack for complex types
- *Use Case*: Enterprise applications, microservices architecture
- *Benefits*: Industry standard, excellent tooling, strong typing
- *Command*: =python src/cli/run_server.py -t step2_grpc -p 50051=

**** FastAPI + JSON-RPC (HTTP-based RPC)
- *Location*: =src/step2/fastapi_jsonrpc/=
- *Protocol*: HTTP with JSON-RPC 2.0 + base64-encoded MessagePack
- *Use Case*: Web-friendly applications, REST API familiarity
- *Benefits*: HTTP standard, easy debugging, web ecosystem compatibility
- *Command*: =python src/cli/run_server.py -t step2_fastapi_jsonrpc -p 8000=

*** Shared Architecture

All Step 2 implementations share:
- *Common Serialization*: =src/base/msgpack_helpers.py= handles complex banking datatypes
- *Identical Interface*: All implement =base.proxy_api.OpsProxyAPI= for seamless interchangeability
- *Same Banking Logic*: All use =base.server.Server= for business operations
- *Unified CLI*: Same command-line tools work with all transports via =src/cli/transports.py=

*** Testing All Transports

Each transport can be tested with the same banking operations. Look in requirements for the
requirements.txt that matches to transport you choose to try.

#+BEGIN_SRC bash
# Test aiozmq transport
python src/cli/run_server.py -t step2_aiozmq -p 55555 &
python src/cli/run_client.py -t step2_aiozmq -p 55555

# Test gRPC transport  
python src/cli/run_server.py -t step2_grpc -p 50051 &
python src/cli/run_client.py -t step2_grpc -p 50051

# Test FastAPI JSON-RPC transport
python src/cli/run_server.py -t step2_fastapi_jsonrpc -p 8000 &
python src/cli/run_client.py -t step2_fastapi_jsonrpc -p 8000
#+END_SRC

*** Educational Value

These implementations demonstrate:
- *RPC Design Patterns*: How to adapt the same business logic to different transport mechanisms
- *Serialization Strategies*: Handling complex Python datatypes across network boundaries
- *Async Programming*: All implementations are fully async-compatible
- *Production Readiness*: Each approach is suitable for real-world Raft server communication

*** Choosing a Transport for Raft

For the final Raft integration (Major Step Three), consider:
- *gRPC*: Best for enterprise environments, strong typing, excellent tooling
- *FastAPI + JSON-RPC*: Most familiar to web developers, easy debugging
- *aiozmq*: Highest performance for server-to-server communication

All three are suitable for Raft server-to-server communication and will be carried forward
to demonstrate Raft message passing in the final implementation stage.

