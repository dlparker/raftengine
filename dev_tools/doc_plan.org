
* Purpose

To generate documents in the sphinxdoc source directory that detail the actions and meaning
of the integtation tests in the tests directory. They aren't really unit tests, except for
a small number of them, rather they test the library code by building up a simulated network
of server nodes that call the actual library functions in the exact same way an actual
implementation would.

The goal of these test documents is to detail what each test does in terms of the raft
protocol implementation used or tested by the test. By "used" I mean that operations
that are used to prepare the test cluster for the actual test may be raft operations.
For example, nearly all the tests run elections. Only the election tests test that
code, so other tests are said to "use" it. It is helpful to track those uses so
the reader can understand the state of the server under test conditions.

* Current state of the document generators

** Tracing
The test support code in dev_tools trace a lot of what is happining during tests,
generally triggering off of the passing of messages from one node to the other.
The following files are the main ones involved in this:

1. pausing_cluster.py
2. pausing_server.py
3. network_sim.py
4. test_trace.py TestTrace class in particular
5. trace_data.py

   
They coordinate to produce an in memory array (python list) of records of the
state of each node in the cluster whenever something of interest has been
noted, as encoded in the SaveEvent class in trace_data.py.

The individual test functions and the TestTrace code also save a range
of information about the test and the sections of the test. Some of this
comes from the test function's doc string, some of it comes from calls
made by the test functions to define the test, the test sections, and
the raft features used and tested but each section. Currently
the feature definitions process is limited to some examples created for that
purpose in tests/test_random_code.py. 

** Trace saving
When a test runs, the cleanup step of the test fixture calls the
TraceOutput class (dev_tools.trace_output.py) which writes out
the trace array as json, along with all the TestTrace context
about the tests and sections.

The raft feature definitions are stored in a sqlite db by
dev_tools/feature_db.py so that it can build up tables
cross referencing features and tests.

** File generation

The file dev_tools/build_docs.py converts all available test trace json
files to documents in a variety of formats. These generated files
are placed in the "captures/test_trace" directories

| Type          | Format                                                   | Location                          |
| CSV           | comma separated rows of every nodestate trace records    | captures/test_trace/csv `         |
| CSV digest    | comma separated rows of filtered nodestate trace records | captures/test_trace/digest_csv    |
| org           | Org formatted test report with traces in tables          | captures/test_trace/org           |
| org no legend | Like org formatted but without the table legend          | captures/test_trace/no_legend_org |
| rst           | Rst formatted test report with traces in tables          | captures/test_trace/rst           |
| plantuml      | Plantuml diagrams for each test section                  | captures/test_trace/plantuml      |

The CSV files do not contain the any of the additional information about the tests, test sections or features
that are collected to use in the report files.

The "org no legend" files are intended to be combined into a single org doc and converted to pdf to produce
a comprehensive report of the entire test suit.

The "rst" and "plantuml" are to be combined with other information such as raft feature documentation files
to produce a page in the project's documentation tree for each test.


** Feature definition process

If a test is up to date with the new feature definition tools it works with the TestTrace class to mark
each test section to indicate which raft features it uses and/or tests. The marking process also
drives the defition process in dev_tools/features.py. By convention the main Raft concept is a
"feature" and the details of how that concept applies to a scenario are feature branches.

For example, there currently defined features include the "state_machine_command" command
feature, which is an umbrella for all the Raft protocol guarantees and restrictions
surrounding the replication of a client command to all the nodes in the cluster.

The simplest case of this feature occurs with all the nodes are keeping up with log replication
(another feature) and are ready to immediately replicate the new command when the leader sends
it. This gets the branch name "all_in_sync", so it has a full specification of:

state_machine_command.all_in_sync

A different scenario occurs when one (or more) of the followers is slow for some reason and
does not already have a fully up to date log when the new command log record arrives
from the leader. This is gets the branch name "apply_on_delayed_replication", so the
full name is:

state_machine_command.apply_on_delayed_replication

These name and the concepts they represent are arbitrary and derive from the structure of
the individual tests that use them. For example, the test test_feature_defs_3 in
tests/test_random_code.py uses the following feature branches:

+----------------------------------------------------+---------------+--------------+
|                        path                        | section_index | relationship |
+----------------------------------------------------+---------------+--------------+
| leader election.all_yes_votes.with_pre_vote        | 0             | uses         |
| state machine command.all_in_sync                  | 1             | tests        |
| state machine command.request_redirect             | 2             | tests        |
| state machine command.minimal_node_count           | 5             | tests        |
| log_replication.slow_follower_backdown             | 6             | uses         |
| state machine command.apply_on_delayed_replication | 6             | tests        |
+----------------------------------------------------+---------------+--------------+

The test calls the FeatureRegistry to get or create the features and branches it
needs, then passes these to the TestTrace class to mark the subtests (sections)
with the used or tested feature branches.


** Feature document fragments

When a feature is used or tested in a test, and then dev_tools/build_docs.py
is run, the auto generation of the rst format test report inserts references
to rst fragment files that document features. 

When a feature or branch is added to the database during a test run, the
FeatureRegistry ensures that each feature and path has a set of stub
files in the captures/features/doc tree. These represent the file
targets that will be inserted into the generated rst files.

As an  example, if you run the test tests/test_random_code.py::test_feature_defs_3
the following lines will execute:

.. code-block:: python
    f_election = registry.get_raft_feature("leader_election", "all_yes_votes.with_pre_vote")
    spec = dict(used=[f_election,], tested=[])
    await cluster.test_trace.start_test_prep("Running normal election till fully replicated", features=spec)

The results of these calls will be reflected in the features database in captures/features/features.db
and in the json trace file captures/test_traces/json/test_random_code/test_feature_defs_3.json that
is created at the end of the test run.

When the dev_tools/build_docs.py tool is run it loads the test trace,
looks up features used in the test, and ensures that the there are
stub file created for each of the document fragements expected in
the doc tree. The following is an incomplete list of the files
so created for test_random_code.py::test_feature_defs_3
 
captures/features/docs/leader_election/narative.rst
captures/features/docs/leader_election/features.rst
captures/features/docs/leader_election/short.rst
captures/features/docs/leader_election/branches/all_yes_votes/narative.rst
captures/features/docs/leader_election/branches/all_yes_votes/features.rst
captures/features/docs/leader_election/branches/all_yes_votes/short.rst
captures/features/docs/leader_election/branches/all_yes_votes/with_pre_vote/narative.rst
captures/features/docs/leader_election/branches/all_yes_votes/with_pre_vote/features.rst
captures/features/docs/leader_election/branches/all_yes_votes/with_pre_vote/short.rst

The dev_tools/build_docs.py tool also generates the rst file for the test
in captures/test_traces/rst/test_random_code/test_feature_defs_3.rst
and includes references to the fragment files:

Raft features used:

.. include:: /developer/tests/features/leader_election/short.rst

.. collapse:: leader_election details (click to toggle view)

   .. include:: /developer/tests/features/leader_election/features.rst

   .. include:: /developer/tests/features/leader_election/narative.rst


.. include..  :: /developer/tests/features/leader_election/branches/all_yes_votes/short.rst

.. collapse:: leader_election/branches/all_yes_votes details (click to toggle view)

   .. include:: /developer/tests/features/leader_election/branches/all_yes_votes/features.rst

   .. include:: /developer/tests/features/leader_election/branches/all_yes_votes/narative.rst


.. include..  :: /developer/tests/features/leader_election/branches/all_yes_votes.with_pre_vote/short.rst

.. collapse:: leader_election/branches/all_yes_votes.with_pre_vote details (click to toggle view)

   .. include:: /developer/tests/features/leader_election/branches/all_yes_votes.with_pre_vote/features.rst

   .. include:: /developer/tests/features/leader_election/branches/all_yes_votes.with_pre_vote/narative.rst



** TODO
Automate current manual steps, but be careful not to overwrite and lose manual edits.

copy captures/test_traces/rst/test_path*/test_name*.rst to docs/source/developer/tests/[elections|commands|snapshots|etc]

copy captures/test_traces/plantuml/test_path*/test_name*.puml to docs/source/developer/tests/diagrams/test_path

check captures/docs/* and see if there are matching files in docs/source/developer/test/features
and if so *DO NOT OVERWRITE*. if file does not exist in target complain to get human to write it.

   
** TODO

There are first drafts of the needed files for the three tests
in test_random_code.py, they can be found in docs/source/developer/tests/features in
a directory structure matching the layout of captures/features/docs.

** TODO
There should be a command line tool, or possibly an option to the
build_docs.py script that checks to see if the actual doc tree
version of a file exists to match the generated files in
captures/features/docs and prints out a list of the missing
files as a guide to the work remaining.





** TODO
Much work remains to be done to choose the feature definitions needed for remaining
tests, something like 73 or 74 tests out of the 77 in the suite (not counting the
ones in tests/test_random_code.py, which are temporary). This will most likely
have to be done by a human rather than an LLM/Agent combination, as so much is
uspoken but expected about the results.

