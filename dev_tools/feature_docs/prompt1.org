In a previous chat with you we discussed elements of my documentation enhancement
efforts for my Raft Library project found at https://github.com/dlparker/raftengine.git.
I had to terminate that session because your context had reached the point where old
prompts and there results were poluting your responses as I tried to change directions
with new prompts.

One of the things we clarified is that your knowledge of the availability of the Grok API
is out of date. I am able to use it and have set up a VSCode and Cline configuration that
has worked for a real test task. So when I tell you that you can give me instructions to
feed back to you through Cline and your API, you can accept that I will make it work.

My project includes an extensive test suit and I have been working on generating test
documentation from it. The test system includes a tracing mechanism defined mostly in
dev_tools/test_trace.py and you have enhanced this mechamism to produce plantuml files
as generated from the test trace data. There is some additional information about how
that works in the attached test_trace_desc.txt file. I have attached and example auto
generated test description rst file and the matching auto generated plantuml file (saved
as puml.txt to get web ui to accept it). These were generated by TestTrace.save_rst()
and TestTrace.save_plantuml in dev_tools/test_trace.py respectively.

The current focus of my efforts is to improve and complete my efforts to explain
the relationship between the test operations and the element of the Raft Protocol
as described in Diego Ongoro's Raft Protocol thesis.  


There are some unfinished elements that need to be upgraded or replaced.
I had two goals for these elements:

1. To make it easier for interested humans to understand what parts of the Raft features are used in a test.
   1. Used to setup the test operations (for example, running an election to set for testing command propagation)
   2. The Raft features that are the focus of the test.
2. The make develop a table of compliance that links the Raft features from the thesis with the test(s) that
   use and focus on the feature

For the first goal, I want to provide a condensed discussion of which Raft features are the focus of the test,
which often involves many separate elements of the thesis. Also part of this goal is to provide references
into the parts of the thesis that define these elements or features.

For the second goal, I was imagining something like the following. This is a rough idea that needs work, but
the goal would be to allow someone to read the thesis, and for any given part of the thesis use this table
to look up the relevant tests and clock through to the associated test docs. 

| Feature                               | Focus Tests                      | Used by Tests                                                      |
| Election in a new cluster, empty logs | test_election_1, test_election_2 | All tests except for test_member_change_messages, test_str_methods |



The code that I currently have in dev_tools/features.py combined with the "define_test" method on the TestTrace class in
dev_tools/test_trace.py are an embryonic attempt to address goal one. My plan was to come up with short text tags that
referenced a FeatureDesc item as registered in RaftUseDefs. The FeatureDesc item would then provide hand coded explainations
that could then be placed in the output .org and .rst files.

For a variety of reasons I no longer favor this plan. Here is an outline for what I would like to build to replace it:

2. Build a replacement for dev_tools/features.py and discard all usage of that file's contents.
   1. I have created a "feature_docs" subdirectory under dev_tools
   2. Create feature_docs files using this workflow:
      1. Run a test, and call TestTrace.save_rst at the end (note that this is controlled by global variables in dev_tools/pausing_cluster.py)
      2. Submit the resulting .rst file to you for you to build a thesis mapping for it and a description of
         the Raft features used in the test as you see it.
         1. Check the dev_tools/features directory to see if there is already a set of files for this feature
	    1. If there is, continue
	    2. If there isn't, generate a name for the feature and the .rst and .org files so named containing your results
	       1. The name for the feature should be some unqiue id string that gives humans a clue what the feature is about,
                  so something like "candidate_loses_election_since_log_not_latest" or "leader_loses_relection_after_partition_heal".
                  There may be cases where tests are too similiar to make this string a reasonable length, in which case the
                  string could be something like "complex_feature_case_1" and "complex_feature_case_2".
	    3. NOTE: there needs to be a mechanism here to allow inclusion of text that I write and place in this
	       directory that will be included in the generated .rst and .org files and that will not be overwritten if
	       this workflow is re-run. Maybe something like FEATURE_UNIQUE_ID_STRING.header.org?
         2. Create or update the TEST_NAME_features.json file to indicate that the feature is used by the test. It
	    should contain  a dictionary containing two items named "uses" and "focused", each of which is a list of
	    file names (not paths, just the leaf name and only the stem, not the .rst or .org extension).
	    If you can't tell whether the feature is the focus of the test or not, just add it to the "used" list.
	    We may be back later to improve the TestTrace logic to have some kind of marker to highlight what is
	    being tested as opposed to being used to create  test conditions.
3. Update TestTrace to use new feature_docs stuff
   1. update TestTrace save_org and save_rst methods to look for TEST_NAME_features.json in the
      feature_docs subdirectory.
   3. Parse that json file if found and find files in the feature_docs subdirectory and include them:
      1. If in the "uses" item, just include a reference to another doc. The path to this doc directory should
	 be configurable and should default to "../raft_features/", which will correspond to docs/source/tests/raft_features.
      2. If in the "focused" item, then the text in the file should be directly inserted into the output file, using the
	 a path composed of dev_tools/feature_docs/REFERENCED_FILE_STEM.org or .rst as appropriate.


When all this is done, it should be relatively simple for you to generate the lookup table that satisfies
goal number 2, though we will probably need to iterate on the form of the table until it is really reader
friendly. Indeed, it is possible that a table is the wrong format, that it should be a more normally structured
.rst file with a subsection or something for each feature and the references to tests as bulleted lists. Don't
know, let's find out.


So, what I would like specifically from this request is a plan that:

1. Gives me an instruction to submit to your API through cline and VSCode that will:
   1. Run a single test (e.g. ./run_tests.sh tests/test_elections_1.py::test_election_1)
   2. Find the resultant .rst file in captures/test_traces (e.g. captures/test_traces/rst/test_elections_1.rst)
      and run the features analsys on this test and produce the features files.
2. Propose a format for the feature to test mapping rst file.
	 
	 
   
	 

      
      
   
