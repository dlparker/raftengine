* immediate

1. Work over log levels used in leader, candidate, follower, deck so that a clean but useful
   log output is made when selecting that level for those loggers.

   
NOW!!!!

1. Change Commit index to Applied index in all snapshot stuff
   
1. [ ] Document updates to reflect updated snapshot create logic:
   1. Deck gets take_snap call await deck.take_snapshot(timeout) 
   2. Deck does role stop (transfer if leader)
   3. Deck collects log commit_index and term of that record
   4. Deck calls pilot await pilot.create_snapshot(snap_index, snap_term)
   5. Pilot creates the snapshot
      1. In our test code it calls the operations class to do that
      2. Operation class saves snapshot (in memory)
   6. Pilot calls operations returns snapshot to deck 
   7. Deck calls log to install snapshot
      1. Saves snap_index, snap_term in db
      2. Deletes all records up to an including snap_index
   8. Deck returns snapshot to app code
2. [ ] Clarify documentation about snapshot export
   1. Follower recieves snapshot start message from leader (offset = 0)
   2. Follower generates a SnapShot instance and calls PilotAPI.begin_stanshot_import,
      which returns a SnapShotToolAPI
   3. Follower uses tool to install first chunk
   4. Follower replies to leader
   5. Repeats installing chunks until message says "done"
   6. Follower calls log_api install_snapshot
   7. Log saves snapshot properties, then deletes all records up to and
      including the snapshot index.
   8. Follower reply implies that snapshot is installed and log trimmed.
3. [ ] Clarify documentation about snapshot export
   1. Leader code detects need to send snapshot to follower (why?)
   2. Leader code retrieves snapshot from log, which is just index and term
   3. Leader calls PilotAPI.begin_snapshot_export(snapshot)
   4. Pilot implementation figures out how to find snapshot data, creates a SnapShotToolAPI
	 to return to Leader.
   5. Pilot uses tool to collect chunks of snapshot data and send to follower, repeating till
      all sent.
      
      
      
      
      
	 

3. [X]  Fix snapshot build sequence, log can move forward between start at operations and stop at role.
   Should be:
   1. Deck gets take_snap call await deck.take_snapshot(timeout) REMOVE the snapshot argument
   2. Deck does role stop (transfer if leader)
   3. Deck collects log index, term
   4. Deck calls pilot await pilot.create_snapshot(last_index, last_term)
   5. Pilot creates the snapshot, and in our example it calls the operations class to do that.
   6. Pilot calls operations take_snapshot, returns updated snapshot to deck 
   7. Deck calls log to install snapshot 
   8. Deck returns snapshot to app code (not there yet, prolly callback)
4. [ ] Change the logic for snapshot tool, make it requried, but supply a tool child
   class that saves chunks to log, json encoding them if so configured, show
   an example of how to use it.
5. [ ] pUpdate documentation, including pumls. Simplest  way is to add a "snapshot_store"
   and have tool call that instead of app db, then somehow document that the
   snapshot_store can be the log db.

6. [X] Add cluster config as term start message data, that ensures that new node joining a cluster
   with no config changes applied can look up the correct config in the last term start.
7. [X] Add logic to follower to apply cluster config as present in first term start log record
8. [X] Plan a way to share changes to cluster config settings. Shouldn't need undo, so kind of an
   admin command, might want to generalize it that way so other commands are easy (e.g. logging control)
9. [] build an example mangement interface for doing cluster ops such as config, snapshots.
10. [] Think about serialization. Currently command and command replies have to be strings, which is yucky.
    This could be fixed by providing an interface for serialize/deserialize but need to think about how
    serializing messages and serializing log record contents are different. Maybe the log_api can accept
    a seriailization object, and the message ops leave it to the Pilot? Fix the various run command
    annotations and docs if done.
11. [] Need to think about changing the Pilot and Deck api message functions to use something like
    {msg_id: 10, payload: LONG_STRING} because it will be hard to implement reply_to functionality
    if the message is just and encoded string. How to make it not need perisistence? Could use UUID.

   
   
* Snapshots

1. [] Document that make_snapshot requires node to be "offline". Note how you could keep it online
   during snapshot creation, but how loading a snapshot must be offline. Fresh server case is clear,
   but what about a slow server that has some records? Can leader force it offline? Maybe sending
   a snapshot chunk message does that? So maybe there's a new role, not follower but slow_follower?
   Can existing new node loading code in leader treat this situation as the same a new load?
   Maybe leader just says, oh, I have to send snapshot to this follower, so I'll call it new, and
   the follower sees the snapshot message and says "oh, all my state is bad, clear the log" and
   when snapshot tool sees new incoming snapshot it invalidates the current state?
   Leader stops sending broadcasts to server that is marked as receiving snapshot
2. [X] Need some way to re-install snapshot on restart. Maybe we just store the snapshot, minus
   the tool reference in the log, then when we pass the snapshot to the pilot for export it fills
   the tool reference. This should work as it does in memlog, just need to implement and test
   it with sqlite log.
   
     
   
Events and errors rework:
1. [X] Fix Pilot and Deck interaction for events, deck and deck_api need to expose them, Pilot should probably
   have a required method for major events, remove and add events need to be added to major list.
2. [X] Need to cleanup message_problem_history code and make a better effort at error reporting, see event item above
3. [x] completely convert substate to events
4. [x] Need to have self exit generate an event.


1. [X] Need to update leader commit voting logic to not self count if exit_in_progress so that commit gets majority without
2. [X] Need to figure out what it means to abort remove node. Reverse would only happen when leader crashes with
   uncommitted and followers don't have it yet, then election overwrites the index. So when leader restarts, it
   gets an overwrite from new leader on that record, and it has to undo the temporary change. It can't notify the
   original requester, because it does not have the info about who that was. 
   new cluster commits new stuff). So that means that deleting records on leader command needs to see if any of them
   are membership changed records, and undo the change. That's probably true for both types. It means follower can
   no longer just blindly delete, it has to inspect and dispose.
   Can a remove record hang the config forever? 
3. [X] Need to update cluster change calls so that they only return when complete, like command runner. That way
   caller can know that the change is complete. Consider a callback or event instead
4. [X] Update follower remove op to wait for commit, and update leader to sent heartbeat to removing server. Follower logic
   probably needs to ignore heatbeats unless commit equals the record index of the remove log entry. Avoid overly complex
   logic to prevent messing wwith ongoing log replication.
   


* Protocol

1. [X] Need to implement lastApplied, and make sure that commit index is set before command is run. Currently
   I am using commitIndex as though it were last applied. The idea that commit comes before apply is strange
   to me, but that is what it says. Prolly need some tests for failure between commit index incr and
   last applied incr.
2. Look in paper and thesis to see about command sequence number or id, I think it is in there.
3. [X] Need PreVote and CheckQuorum: https://dev.to/tarantool/raft-notalmighty-how-to-make-it-more-robust-3a11
  https://github.com/ongardie/dissertation?tab=readme-ov-file
  https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/
  Want to implement them as opt in features, segregate tests 
4. [X] Cluster membership change protocol
5. [X] Snapshots


* Test questions and issues and Demo issues

1. [X] Develop a plan for doing tracing in real processes, which means adding event callback support to the library.
   Maybe build an event dispatch dictionary so tracing points can be efficient. If there is a dict, then call
   a function that dispatches to traces, maybe passing locals() for context. That function can collect log data, message
   data, etc. Replace substate calls with this. Do the traces as JSON with class names in them so that it can
   be stored and reconstituted. Maybe make this an option, if the system works kind of like python logging, collection,
   filters, handlers, etc. Incorporate concerns below about error reporting. Maybe one event system for errors, and
   another just like it for non-errors so that they can be efficiently disabled.
   Earlier note with same intent:  Find all places where servers.py and tests open the white box and replace them with event
   generation and delivery to the pilot interface.
   1. An event class with an emum type
      1. Error
      2. Role Change
      3. Term Change
      4. Message In
	 1. some kind of filtering so that we can avoid eventing every message
      5. Message Out
	 1. some kind of filtering so that we can avoid eventing every message
      6. Message Summary (maybe publish and clear when heartbeat sent or received, or log index delta > threshold)
      7. Log Index Change
      8. Log Term Change
	 
	 
	 
 




