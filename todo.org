* immediate

1. [X] Add cluster config as term start message data, that ensures that new node joining a cluster
   with no config changes applied can look up the correct config in the last term start.
2. [X] Add logic to follower to apply cluster config as present in first term start log record
3. [] Plan a way to share changes to cluster config settings. Shouldn't need undo, so kind of an
   admin command, might want to generalize it that way so other commands are easy (e.g. logging control)
   
* Snapshots

1. [] Document that built in method requires node to be "offline". Note how you could keep it online
   during snapshot creation, but how loading a snapshot must be offline. Fresh server case is clear,
   but what about a slow server that has some records? Can leader force it offline? Maybe sending
   a snapshot chunk message does that? So maybe there's a new role, not follower but slow_follower?
   Can existing new node loading code in leader treat this situation as the same a new load?
   Maybe leader just says, oh, I have to send snapshot to this follower, so I'll call it new, and
   the follower sees the snapshot message and says "oh, all my state is bad, clear the log" and
   when snapshot tool sees new incoming snapshot it invalidates the current state?
   Leader stops sending broadcasts to server that is marked as receiving snapshot
2. [] Need some way to re-install snapshot on restart. Maybe we just store the snapshot, minus
   the tool reference in the log, then when we pass the snapshot to the pilot for export it fills
   the tool reference.
   
     
   
Events and errors rework:
1. [] Fix Pilot and Hull interaction for events, hull and hull_api need to expose them, Pilot should probably
   have a required method for major events, remove and add events need to be added to major list.
2. [] Need to cleanup message_problem_history code and make a better effort at error reporting, see event item above
3. [x] completely convert substate to events
4. [x] Need to have self exit generate an event.


1. [x] Need to update leader commit voting logic to not self count if exit_in_progress so that commit gets majority without
2. [x] Need to figure out what it means to abort remove node. Reverse would only happen when leader crashes with
   uncommitted and followers don't have it yet, then election overwrites the index. So when leader restarts, it
   gets an overwrite from new leader on that record, and it has to undo the temporary change. It can't notify the
   original requester, because it does not have the info about who that was. 
   new cluster commits new stuff). So that means that deleting records on leader command needs to see if any of them
   are membership changed records, and undo the change. That's probably true for both types. It means follower can
   no longer just blindly delete, it has to inspect and dispose.
   Can a remove record hang the config forever? 
3. [x] Need to update cluster change calls so that they only return when complete, like command runner. That way
   caller can know that the change is complete. Consider a callback or event instead
4. [x] Update follower remove op to wait for commit, and update leader to sent heartbeat to removing server. Follower logic
   probably needs to ignore heatbeats unless commit equals the record index of the remove log entry. Avoid overly complex
   logic to prevent messing wwith ongoing log replication.
   


* Protocol

1. [X] Need to implement lastApplied, and make sure that commit index is set before command is run. Currently
   I am using commitIndex as though it were last applied. The idea that commit comes before apply is strange
   to me, but that is what it says. Prolly need some tests for failure between commit index incr and
   last applied incr.
2. Look in paper and thesis to see about command sequence number or id, I think it is in there.
3. [X] Need PreVote and CheckQuorum: https://dev.to/tarantool/raft-notalmighty-how-to-make-it-more-robust-3a11
  https://github.com/ongardie/dissertation?tab=readme-ov-file
  https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/
  Want to implement them as opt in features, segregate tests 
4. [X] Cluster membership change protocol
5. [] Snapshots


* Test questions and issues and Demo issues

1. [X] Develop a plan for doing tracing in real processes, which means adding event callback support to the library.
   Maybe build an event dispatch dictionary so tracing points can be efficient. If there is a dict, then call
   a function that dispatches to traces, maybe passing locals() for context. That function can collect log data, message
   data, etc. Replace substate calls with this. Do the traces as JSON with class names in them so that it can
   be stored and reconstituted. Maybe make this an option, if the system works kind of like python logging, collection,
   filters, handlers, etc. Incorporate concerns below about error reporting. Maybe one event system for errors, and
   another just like it for non-errors so that they can be efficiently disabled.
   Earlier note with same intent:  Find all places where servers.py and tests open the white box and replace them with event
   generation and delivery to the pilot interface.
   1. An event class with an emum type
      1. Error
      2. Role Change
      3. Term Change
      4. Message In
	 1. some kind of filtering so that we can avoid eventing every message
      5. Message Out
	 1. some kind of filtering so that we can avoid eventing every message
      6. Message Summary (maybe publish and clear when heartbeat sent or received, or log index delta > threshold)
      7. Log Index Change
      8. Log Term Change
	 
	 
	 
2. [] Think about building an example mangement interface for doing cluster ops such as config, snapshots.
 




