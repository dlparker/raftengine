* immediate

1. Work over log levels used in leader, candidate, follower, hull so that a clean but useful
   log output is made when selecting that level for those loggers.

   
NOW!!!!
1. Fix snapshot build sequence, log can move forward between start at operations and stop at role.
   Should be:
   a. Hull gets take_snap call
   b. Hull does role stop (transfer if leader)
   c. Hull calls pilot take_snapshot
   d. Pilot calls operations take_snapshot, returns snapshot to hull
   e. Hull calls log to install snapshot
   f. Hull returns snapshot to app code (not there yet, prolly callback)


1. [X] Add cluster config as term start message data, that ensures that new node joining a cluster
   with no config changes applied can look up the correct config in the last term start.
2. [X] Add logic to follower to apply cluster config as present in first term start log record
3. [X] Plan a way to share changes to cluster config settings. Shouldn't need undo, so kind of an
   admin command, might want to generalize it that way so other commands are easy (e.g. logging control)
4. [] build an example mangement interface for doing cluster ops such as config, snapshots.
5. [] Think about serialization. Currently command and command replies have to be strings, which is yucky.
   This could be fixed by providing an interface for serialize/deserialize but need to think about how
   serializing messages and serializing log record contents are different. Maybe the log_api can accept
   a seriailization object, and the message ops leave it to the Pilot? Fix the various run command
   annotations and docs if done.
6. [] Need to think about changing the Pilot and Hull api message functions to use something like
   {msg_id: 10, payload: LONG_STRING} because it will be hard to implement reply_to functionality
   if the message is just and encoded string. How to make it not need perisistence? Could use UUID.

   
   
* Snapshots

1. [] Document that make_snapshot requires node to be "offline". Note how you could keep it online
   during snapshot creation, but how loading a snapshot must be offline. Fresh server case is clear,
   but what about a slow server that has some records? Can leader force it offline? Maybe sending
   a snapshot chunk message does that? So maybe there's a new role, not follower but slow_follower?
   Can existing new node loading code in leader treat this situation as the same a new load?
   Maybe leader just says, oh, I have to send snapshot to this follower, so I'll call it new, and
   the follower sees the snapshot message and says "oh, all my state is bad, clear the log" and
   when snapshot tool sees new incoming snapshot it invalidates the current state?
   Leader stops sending broadcasts to server that is marked as receiving snapshot
2. [X] Need some way to re-install snapshot on restart. Maybe we just store the snapshot, minus
   the tool reference in the log, then when we pass the snapshot to the pilot for export it fills
   the tool reference. This should work as it does in memlog, just need to implement and test
   it with sqlite log.
   
     
   
Events and errors rework:
1. [X] Fix Pilot and Hull interaction for events, hull and hull_api need to expose them, Pilot should probably
   have a required method for major events, remove and add events need to be added to major list.
2. [X] Need to cleanup message_problem_history code and make a better effort at error reporting, see event item above
3. [x] completely convert substate to events
4. [x] Need to have self exit generate an event.


1. [X] Need to update leader commit voting logic to not self count if exit_in_progress so that commit gets majority without
2. [X] Need to figure out what it means to abort remove node. Reverse would only happen when leader crashes with
   uncommitted and followers don't have it yet, then election overwrites the index. So when leader restarts, it
   gets an overwrite from new leader on that record, and it has to undo the temporary change. It can't notify the
   original requester, because it does not have the info about who that was. 
   new cluster commits new stuff). So that means that deleting records on leader command needs to see if any of them
   are membership changed records, and undo the change. That's probably true for both types. It means follower can
   no longer just blindly delete, it has to inspect and dispose.
   Can a remove record hang the config forever? 
3. [X] Need to update cluster change calls so that they only return when complete, like command runner. That way
   caller can know that the change is complete. Consider a callback or event instead
4. [X] Update follower remove op to wait for commit, and update leader to sent heartbeat to removing server. Follower logic
   probably needs to ignore heatbeats unless commit equals the record index of the remove log entry. Avoid overly complex
   logic to prevent messing wwith ongoing log replication.
   


* Protocol

1. [X] Need to implement lastApplied, and make sure that commit index is set before command is run. Currently
   I am using commitIndex as though it were last applied. The idea that commit comes before apply is strange
   to me, but that is what it says. Prolly need some tests for failure between commit index incr and
   last applied incr.
2. Look in paper and thesis to see about command sequence number or id, I think it is in there.
3. [X] Need PreVote and CheckQuorum: https://dev.to/tarantool/raft-notalmighty-how-to-make-it-more-robust-3a11
  https://github.com/ongardie/dissertation?tab=readme-ov-file
  https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/
  Want to implement them as opt in features, segregate tests 
4. [X] Cluster membership change protocol
5. [X] Snapshots


* Test questions and issues and Demo issues

1. [X] Develop a plan for doing tracing in real processes, which means adding event callback support to the library.
   Maybe build an event dispatch dictionary so tracing points can be efficient. If there is a dict, then call
   a function that dispatches to traces, maybe passing locals() for context. That function can collect log data, message
   data, etc. Replace substate calls with this. Do the traces as JSON with class names in them so that it can
   be stored and reconstituted. Maybe make this an option, if the system works kind of like python logging, collection,
   filters, handlers, etc. Incorporate concerns below about error reporting. Maybe one event system for errors, and
   another just like it for non-errors so that they can be efficiently disabled.
   Earlier note with same intent:  Find all places where servers.py and tests open the white box and replace them with event
   generation and delivery to the pilot interface.
   1. An event class with an emum type
      1. Error
      2. Role Change
      3. Term Change
      4. Message In
	 1. some kind of filtering so that we can avoid eventing every message
      5. Message Out
	 1. some kind of filtering so that we can avoid eventing every message
      6. Message Summary (maybe publish and clear when heartbeat sent or received, or log index delta > threshold)
      7. Log Index Change
      8. Log Term Change
	 
	 
	 
 




