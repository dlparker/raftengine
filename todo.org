
Events and errors rework:
1. [] Fix Pilot and Hull interaction for events, hull and hull_api need to expose them, Pilot should probably
   have a required method for major events, remove and add events need to be added to major list.
2. [] Need to cleanup message_problem_history code and make a better effort at error reporting, see event item above
3. [] completely convert substate to events
4. [] Need to have self exit generate an event.


1. [] Have self_add just pick a node from cluster config, expecting to get leader_uri back in the change response,
   then update leader_uri and try again.
2. [X] Need to add logic the the bcast_pending code in leader to remove out of date records in case a server
   goes down for a while, could retire on arrival of another message from same server, also on election_timeout
3. [X] refactor servers.py into subunits, setup_log, test_tracing, sim_net, pausing_server, pausing_cluster, pause_triggers, sequences.
   
   
1. [x] Need to update leader commit voting logic to not self count if exit_in_progress so that commit gets majority without
2. [x] Need to figure out what it means to abort remove node. Reverse would only happen when leader crashes with
   uncommitted and followers don't have it yet, then election overwrites the index. So when leader restarts, it
   gets an overwrite from new leader on that record, and it has to undo the temporary change. It can't notify the
   original requester, because it does not have the info about who that was. 
   new cluster commits new stuff). So that means that deleting records on leader command needs to see if any of them
   are membership changed records, and undo the change. That's probably true for both types. It means follower can
   no longer just blindly delete, it has to inspect and dispose.
   Can a remove record hang the config forever? 
3. [x] Need to update cluster change calls so that they only return when complete, like command runner. That way
   caller can know that the change is complete. Consider a callback or event instead
4. [x] Update follower remove op to wait for commit, and update leader to sent heartbeat to removing server. Follower logic
   probably needs to ignore heatbeats unless commit equals the record index of the remove log entry. Avoid overly complex
   logic to prevent messing wwith ongoing log replication.
   


* Protocol

1. [X] Need to implement lastApplied, and make sure that commit index is set before command is run. Currently
   I am using commitIndex as though it were last applied. The idea that commit comes before apply is strange
   to me, but that is what it says. Prolly need some tests for failure between commit index incr and
   last applied incr.
2. Look in paper and thesis to see about command sequence number or id, I think it is in there.
3. [X] Need PreVote and CheckQuorum: https://dev.to/tarantool/raft-notalmighty-how-to-make-it-more-robust-3a11
  https://github.com/ongardie/dissertation?tab=readme-ov-file
  https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/
  Want to implement them as opt in features, segregate tests 
4. [] Cluster membership change protocol
5. [] Snapshots


* Test questions and issues and Demo issues

1. [X] Develop a plan for doing tracing in real processes, which means adding event callback support to the library.
   Maybe build an event dispatch dictionary so tracing points can be efficient. If there is a dict, then call
   a function that dispatches to traces, maybe passing locals() for context. That function can collect log data, message
   data, etc. Replace substate calls with this. Do the traces as JSON with class names in them so that it can
   be stored and reconstituted. Maybe make this an option, if the system works kind of like python logging, collection,
   filters, handlers, etc. Incorporate concerns below about error reporting. Maybe one event system for errors, and
   another just like it for non-errors so that they can be efficiently disabled.
   Earlier note with same intent:  Find all places where servers.py and tests open the white box and replace them with event
   generation and delivery to the pilot interface.
   1. An event class with an emum type
      1. Error
      2. Role Change
      3. Term Change
      4. Message In
	 1. some kind of filtering so that we can avoid eventing every message
      5. Message Out
	 1. some kind of filtering so that we can avoid eventing every message
      6. Message Summary (maybe publish and clear when heartbeat sent or received, or log index delta > threshold)
      7. Log Index Change
      8. Log Term Change
	 
	 
	 
2. [] Think about building an example mangement interface for doing cluster ops such as config, snapshots.
 


* Library Features:

1. rough out ideas for cluster class
   1. Knows about "static config" provided at startup. Can be programatically changed? See thesis
   2. Knows about "dynamic config" resulting from cluster changes while running. See thesis
   3. teach candidate and leader to use it. See thesis
   4. Think about providing an optional uri map implementation for users. If provided, need to think
      about how to use it in API calls to let users know the "real" address
2. Think deep about reporting errors in raft code, and detecting them in testing. Break something
   such as rejected append entries and fiddle. Maybe can use substate stuff to alert to error
   and give pilot an api entry on hull to call to get errors? This is kinda partly done in the
   client command execution code. Maybe we need an Error class set and a local log feature?
   If event callback is implemented, this would be a good way to do it. There is code in follower
   to collect information about handled command errors, as well as exceptions raised, so look at that
   and include delivery of that stuff. Add stuff to test tracing for this too.
3. THINK ABOUT HOW TO MAKE THIS NOT NEEDED: Think about weather it is possible to have a pauses state
   in a live cluster. This would have to be done carfully, but if it were ephemeral shooting nodes
   could clear it in emergencies.
4. THINK ABOUT HOW TO MAKE THIS NOT NEEDED: Separate from full pause, it might be enough to just
   disable new commands, maybe with a timeout and definitely ephemeral so a restart should work.


* Notes on log resync:

Leader keeps nextIndex and matchIndex
nextIndex starts at last_index + 1
matchIndex starts at 0


1. Leader sends heatbeat, no entries
   a. Follower replies success. Leader updates matchIndex to nextIndex - 1
   b. Follower replies failed. Leadder starts backdown
2. Leader sends single log record, can be a new record or a backdown or a catchup
   a. Follower replies success. Leader knows that the message had a (some) record(s)
      because the Follower includes maxIndex in reply. However many records where
      sent and accepted can be calculated by subtracting message prevLogIndex from
      maxIndex. Leader updates matchIndex tp message.maxIndex. It updates
      nextIndex to matchIndex + 1. If nextIndex is less than last_index, start catchup
   b. Follower replies failed. Leader starts backdown
   
Backdown:
1. Leader knows that the Follower does not have the record at message.prevLogIndex.
   It does not know if the Follower has the one before, unless the maxIndex value
   in the reply is less than message.prevLogIndex - 1. In that case the Leader
   sets nextIndex to maxIndex + 2 and sends the record at maxIndex + 1. If maxIndex
   is greater than or equal to message.prevLogIndex,then Leader sets nextIndex
   to message.prevLogIndex and sends message.prevLogIndex - 1. This second case
   will result in the Follower deleting one or more records starting at
   message.prevLogIndex. It may or may not match the new record.

Catchup
Leader starts sending records at message.maxIndex + 1, and sets nextIndex to index after
last message sent.


# Define functions that check to see:
1. The logged state term, last_index, last_term, commit_index, are a legal combination
2. That one legal state can follow another (prevents running backwards when generating, prevents advancing
   last_term but not last_index, last_term <= term)

Then
Generate a set of legal combinations, with differences in each value changing up to three units
    (define logic for this and adjust if needed)

Then:
1. define functions that test two legal log states from above to see if they could be present on two
   servers at the same same, based on the servers role  and network membership
   Can define network membership as 0 = crashed, 1 = majority network, 2,3,4 etc means minority network partition number
   See if it is possible to design and alogythm, or if it has to be table driven. Try hard with the algo.
2. Either document reason that checking each pair is enough, or add a function that compares all three.
3. Generate all the legal combinations from above.

Then:
1. Define the legal actions that can happen to a server that are not RAFT messages "action code"
   10. Becomes leader, follower or candidate
   12. Candidate election timeout, retry
   20. Receives user command (only as leader, no state transitions happen otherwise)
   30. Crash soft (log retained)
   40. Crash hard (log lost)
   50. Switches network from majority(1) to minority 2 or 3 (4 is probably not needed)

Then:
1. define a function that tests to see if one state for three servers could legally follow the previous
   state. This needs to be able to analize whether commit could happen. It needs to understand what
   effect crashes have, and what effects partitions have. So it would look at the "action code". 

Build a tool that uses generated states and network and roles and turns it into a series of messages and actions
from one state to the next.





anomalies: Partition, Crash
victims: Leader, follower, candidate
pre-anomaly cluster state: stable, (one candidate multiple candidates) X (from stable, from leaderless)
pre-anomaly command state: none, some committed, none committed but some active, some committed and some active
in-anomaly Majority network: old leader, no leader, new leader, multiterm election
in-anomaly minority network: all crashed so no action, old_leader, no leader, quiet, try election, multiterm election
in-anomaly command ops: none, actual leader only, ex-leader (isolated) only, 2 leader overlap
in-anomaly config ops: none, add server, remove server
exiting anomaly majority net: stable, election in progress
exiting anomaly minority net: ex-leader alive, followers only, candidate(s) running, recovering server(s)
exiting server log states:

enum cluster_states
     all_stable
     net_split
     some_crashed

enum net pop(ulation):  (one for each current network)
   no quorum
   quorum  (but partial)
   complete
   
enum net state:  (one for each current network)
   starting
   stable	 
   electing (only one candidate)
   contesting_election (multiple candidates)

enum log_state:
     no commands
     committed commands
     pending commands
     pending and committed commands
     
anomaly start:
   net_state
   victim_list (e.g. leader, follower, candidate)
   anomaly type (partition or crash)
   log_replication state: inactive, leader local only, all but victim saved, all but victim committed

anomaly_server_phase:
	server_id
	required role (follower, candidate, leader)
	serial
	net (majority, minority)
	action (crash, stay crashed, restart, change to min network, change to maj network,
	       start election, re-start election, queue command, add server, remove server)

anomaly_cluster_phase:
	list of anomaly_server_phase, server missing implies it has no action

anomaly end: (implies all phases complete)
   This is a checklist tool to ensure anomaly phases did what you think they should do
   server roles dict  (maybe a flag to say just restarted?)
   server log states dict: term, last_index, last_term, optional log tail of X records
   net state 
   
XS													x
| Epoc          | Maj Net State | Min Net State | S1                                  |
| Pre           | stable        | None          | leader, pIndx=1 pTerm=1 term=1 ci=1 |
| Break anomaly | quorum        | None          |                                     |
| Broken        |               |               |                                     |
| Reparing      |               |               |                                     |
| Healed        |               |               |                                     |


